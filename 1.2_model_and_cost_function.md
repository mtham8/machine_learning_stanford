### Model Representation

- _Supervised Learning_: Given the "right answer" for each example in the data
    - Regression Problem: Predict real-valued output
        - training set of housing prices
        - `x` = size in feet
        - `y` = price of home
    - Classification: Discrete-valued output
    - Notation:
        - `m` = Number of training examples
        - `x` = "input" variable/features
        - `y` = "output" variable/"target" variable
        - `(x, y)` = one training example
        - (x<sub>i</sub>, y<sub>i</sub>) = `i`th training example
    - Training Set -> Learning Algorithm -> (size of house `x` -> `h` -> estimated price `y`)
        - where `h` = hypothesis
        - How do we represent `h`? Answer: h(x) = O<sub>0</sub> + O<sub>1</sub>x
        - Linear regression with one variable (`x`). (univariate linear regression)
    - When the target variable that we're trying to predict is continuous, such as in our housing
    example, we call the learning problem a _regression problem_.
    - When `y` can take on only a small number of discrete values (such as if, given the living
    area, we wanted to predict if a dwelling is a house or an apartment), we call it a
    _classification problem_.

### Cost Function

- We can measure the accuracy of our hypothesis function by using a **cost function**. This takes
an average difference (actually a fancier version of an average) of all the results of the
hypothesis with inputs from `x`s and the actual output `y`s.

- Idea: Choose O<sub>0</sub>,O<sub>1</sub> so that `h(x)` is close to `y` for our training examples
`(x, y)`

- Cost function (squared error function):
    - J(O<sub>0</sub>, O<sub>1</sub>) = 1/2 mean of (h(x<sub>i</sub>) - y<sub>i</sub>)<sup>2</sup>
        - 1/2 mean of the squares of the difference between the predicted value and the actual
        value
        - works well with regresson problems
        - The mean is halved (`1/2`) as a convenience for the computation of the gradient
        descent, as the derivative term of the square function will cancel out the `1/2` term.

The goal is to minimize J(O<sub>0</sub>, O<sub>1</sub>)

### Gradient Descent

So we have our hypothesis function and we have a way of measuring how well it fits into the data.
Now we need to estimate the parameters in the hypothesis function. That's where gradient descent
comes in.

- _Gradient descent algorithm_: repeat until convergence
    - Simultaneous update

    ```
    temp0 := O(0) - a(derivative of J(O(0), O(1)))
    temp1 := O(1) - a(derivative of J(O(0), O(1)))
    O(0) := temp0
    O(1) := temp1
    ```

    - The way we do this is by taking the derivative (the tangential line to a function) of our
    cost function. The slope of the tangent is the derivative at that point and it will give us a
    direction to move towards. We make steps down the cost function in the direction with the
    steepest descent. The size of each step is determined by the parameter, `a`/alpha, which is called the learning rate.
    - The direction in which the step is taking is determined by the partial derivative of
    J(O<sub>0</sub>, O<sub>1</sub>). Depending on where one starts on the graph, one could end up
    at different points.

Question:

Suppose O<sub>0></sub> = 1, O<sub>1</sub> = 2, and we simultaneously update O<sub>0</sub> and
O<sub>1</sub> using the rule: O<sub>j</sub> := O<sub>j</sub> + sqroot(O<sub>0</sub>O<sub>1</sub>)
(for j=0 and j=1) What are the resulting values of O<sub>0</sub> and O<sub>1</sub>?

Answer:

O<sub>0</sub> = 1 + sqroot(2), O<sub>1</sub> = 2 + sqroot(2)

### Gradient Descent Intuition

- If `a` is too small, gradient descent can be slow.
- If `a` is too large, gradient descent can overshoot the minimum. It may failt to converge, or
even diverge.

- Gradient descent can converge to a local minimum, even with the learning rate `a` fixed.
- As we approach a local minimum (derivative = 0), gradient descent will automatically take smaller steps. So no need to decrease `a` over time.

### Gradient Descent for Linear Regression

- convex quadratic function (gradient descent): global optimum
- **"batch" gradient descent**: Each step of gradient descent uses all the training examples.




