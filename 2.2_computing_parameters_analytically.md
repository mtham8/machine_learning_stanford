### Normal Equation

- _Normal Equation_: Method to solve for `O`/gradient descent analytically

- In the _Normal Equation_ method, we will minimize `J` by explicitly taking its derivatives with
respect to the `O`j â€™s, and setting them to zero. This allows us to find the optimum theta without
iteration.

- O = (X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>y

- Octave: `pinv(X'*X)*X'*y`

- `m` training examples, `n` features
    - _Gradient descent_
        - need to choose `a`/learning rate
        - needs many iterations
        - PRO: works well even when `n` is large; n = 10<sup>6</sup>
        - PRO: time complexity: O(kn<sup>2</sup>)
    - _Normal equation_
        - no need to choose `a`/learning rate
        - don't need to iterate
        - CON: need to compute (X<sup>T</sup>X)<sup>-1</sup> (time complexity: O(n<sup>3</sup>))
        - CON: slow if `n` is very large; ideally, n <= 10<sup>4</sup>

- There is *no need* to do feature scaling with the normal equation.

### Normal Equation Noninvertibility

- What if X<sup>T</sup>X is non-invertible?
    - _Redundant features_ (linearly dependent)
        - eg. x<sub>1</sub> = size in sqft; x<sub>2</sub> = size in sqm
    - _Too many features_ (eg. `m` <= `n`)
        - delete some features, or use regularization
    - Solutions to the above problems include deleting a feature that is linearly dependent with
    another or deleting one or more features when there are too many features.


