### Multiple Features

- multiple features (variables)
- multivariate linear regression

- notation:
    - m = number of training examples
    - n = number of features
    - x<sub>(i)</sub> = input (features) of `i`th training example
    - x<sub>(i)</sub><sup>j</sup> = value of feature `j` in `i`th training example

- hypothesis:
    - h<sub>O</sub>(x) = O<sub>0</sub> + O<sub>1</sub>x<sub>1</sub> + O<sub>2</sub>x<sub>2</sub> ..

In order to develop intuition about this function, we can think about O<sub>0</sub> as the basic
price of a house, O<sub>1</sub> as the price per square meter, O<sub>2</sub> as the price per
floor, etc. x<sub>1</sub> will be the number of square meters in the house, x<sub>2</sub> the
number of floors, etc.

### Gradient Descent in Practice 1 - Feature Scaling

We can speed up gradient descent by having each of our input values in roughly the same range.
This is because `O` will descend quickly on small ranges and slowing on large ranges, and will
oscillate inefficienty down the optimum when variables are very uneven.

Ideal range: -1 <= x<sub>(i)</sub> <= 1

- _Feature scaling_ involves dividing the input values by the range (ie. the maximum value - the
minimum value) of the input variable, resulting in a new range of just 1.

- _Feature Scaling_: Make sure features are on a similar scale
    - ie. x<sub>1</sub> = size(0-2000 sqft); x<sub>2</sub> = number of bedrooms (1-5)
    - instead, scale the features
    - x<sub>1</sub> = size (sqft)/2000; x<sub>2</sub> = number of bedrooms/5
    - Get every feature into approximately a -1 <= x<sub>i</sub> <= 1 range

- _Mean normalization_ involves substracting the average value for an input variable from the values
for that input variable resulting in a new average value for the input variable of just zero.

- _Mean Normalization_: Replace x<sub>i</sub> with x<sub>i</sub> - u<sub>i</sub> to make features
have approximately zero mean. (Do no apply to x<sub>0</sub> = 1)
    - ie. average house size = 1000
    - then, x<sub>1</sub> = size - 1000/2000; x<sub>2</sub> = bedrooms-2/5

### Gradient Descent in Practice 2 - Learning Rate

- _Debugging gradient descent._ Make a plot with _number of iterations_ on the x-axis. Now plot the
cost function, J(O) over the number of iterations of gradient descent. If J(O) every increases,
then you probably need to descrease `a`.

- For sufficiently small `a`, J(O) should decrease on every iteration.
- If `a` is too small, gradient descent can be slow to converge.
- If `a` is too large, J(O) may not descrease on every iteration; may not converage.

### Features and Polynomial Regression

- Our hypothesis function need not be linear (a straight line) if that does not fit the data well.
- We can *change the behavior or curve* of our hypothesis function by making it a quadratic, cubic,
or square root function (or any other form).
- One important thing to keep in mind is, if you choose your features this way then feature scaling
becomes very important.
